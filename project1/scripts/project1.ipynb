{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import implementations\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#apply method\n",
    "def get_w(y,x,method,initial_w=0,max_iters=10,gamma=0.00000005,log=False,batch_size=1,lambda_=0.005):\n",
    "    initial_=np.array([initial_w]*len(x[0]))\n",
    "    if(method==1):\n",
    "        losses,ws=implementations.least_squares_GD(y,x, initial_, max_iters, gamma, log)\n",
    "        ml=min(losses)\n",
    "        return ml,ws[np.where(losses==ml)[0][0]]\n",
    "    elif(method==2): \n",
    "        losses,ws= implementations.stochastic_gradient_descent(y,x, initial_, max_iters, gamma, batch_size, log)\n",
    "        ml=min(losses)\n",
    "        return ml,ws[np.where(losses==ml)[0][0]]\n",
    "    elif(method==3):\n",
    "        return implementations.least_squares(y,x)\n",
    "    elif(method==4):\n",
    "        return implementations.ridge_regression(y,x, lambda_=lambda_)\n",
    "    else:\n",
    "        return ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss=668544500.6801826, w0=9.43257191142242, w1=10.445648264267412\n(1,)\n(1, 30)\nGradient Descent(): loss=19274197.74733345, w0=9.46436622462608, w1=10.458059526368627\n(1,)\n(1, 30)\nGradient Descent(): loss=1597137.6970529628, w0=9.446477514083504, w1=10.454114610882632\n(1,)\n(1, 30)\nGradient Descent(): loss=6215148.932024098, w0=9.4622322906718, w1=10.464590941476843\n(1,)\n(1, 30)\nGradient Descent(): loss=2293134.0988490963, w0=9.43228443042943, w1=10.459512880514488\n(1,)\n(1, 30)\nGradient Descent(): loss=643439.7356035946, w0=9.438024076598559, w1=10.464224073205688\n(1,)\n(1, 30)\nGradient Descent(): loss=6888455.435749958, w0=9.446220310241019, w1=10.474281923089148\n(1,)\n(1, 30)\nGradient Descent(): loss=84844093.40932347, w0=9.37997499162332, w1=10.454181472019323\n(1,)\n(1, 30)\nGradient Descent(): loss=7631548.643385107, w0=9.33718731185938, w1=10.434152863891402\n(1,)\n(1, 30)\nGradient Descent(): loss=103705449.36511293, w0=9.245754812382247, w1=10.424409348286435\n(1,)\n(1, 30)\nGradient Descent(): loss=1865321.8221649376, w0=9.14927701587335, w1=10.432550664325971\n(1,)\n(1, 30)\nGradient Descent(): loss=845509.5566359897, w0=9.15618229196133, w1=10.433216464982646\n(1,)\n(1, 30)\nGradient Descent(): loss=53073496.30558359, w0=9.083292796362374, w1=10.43010451443377\n(1,)\n(1, 30)\nGradient Descent(): loss=39144629.39832637, w0=9.027276664676268, w1=10.388579836577009\n(1,)\n(1, 30)\nGradient Descent(): loss=79927311.89811817, w0=8.96104440846123, w1=10.356334836168873\n(1,)\n(1, 30)\nGradient Descent(): loss=3405074.2103407453, w0=8.939851895805244, w1=10.356222491764948\n(1,)\n(1, 30)\nGradient Descent(): loss=2560368.9719044203, w0=8.928601864800815, w1=10.353631126381316\n(1,)\n(1, 30)\nGradient Descent(): loss=6963161.464046627, w0=8.946232375459441, w1=10.357394645558191\n(1,)\n(1, 30)\nGradient Descent(): loss=35612673.45832184, w0=8.892108103135232, w1=10.356754086668806\n(1,)\n(1, 30)\nGradient Descent(): loss=1607808.6407650579, w0=8.884286281977058, w1=10.35461505107227\n(1,)\n(1, 30)\nGradient Descent(): loss=73998.57102582835, w0=8.887175274121143, w1=10.356042591316978\n(1,)\n(1, 30)\nGradient Descent(): loss=15175.854767261846, w0=8.888173330878534, w1=10.356144464953497\n(1,)\n(1, 30)\nGradient Descent(): loss=2763116.1617075563, w0=8.900279802321606, w1=10.363869291502397\n(1,)\n(1, 30)\nGradient Descent(): loss=4618832.810885144, w0=8.872697219491595, w1=10.36231861368882\n(1,)\n(1, 30)\nGradient Descent(): loss=410555.1472627698, w0=8.867365288942963, w1=10.359034992991\n(1,)\n(1, 30)\nGradient Descent(): loss=205102.38113161147, w0=8.872073721028842, w1=10.359391095587716\n(1,)\n(1, 30)\nGradient Descent(): loss=63071388.30456594, w0=8.821768031786121, w1=10.320283042489935\n(1,)\n(1, 30)\nGradient Descent(): loss=1670052.1365670166, w0=8.830147829990246, w1=10.321740184155562\n(1,)\n(1, 30)\nGradient Descent(): loss=52875055.54370323, w0=8.766109469515406, w1=10.296600142748717\n(1,)\n(1, 30)\nGradient Descent(): loss=221591994.33230534, w0=8.673283065083119, w1=10.271670440719255\n(1,)\n(1, 30)\nGradient Descent(): loss=105789.14304564722, w0=8.67118058196743, w1=10.270506677435217\n(1,)\n(1, 30)\nGradient Descent(): loss=33491864.927178208, w0=8.262371857200035, w1=10.304657142408203\n(1,)\n(1, 30)\nGradient Descent(): loss=9275147.1166977, w0=8.047236944905315, w1=10.31963044616381\n(1,)\n(1, 30)\nGradient Descent(): loss=161670556.80959836, w0=7.8878697370475015, w1=10.288542804370351\n(1,)\n(1, 30)\nGradient Descent(): loss=56137216.76898288, w0=7.821539008182876, w1=10.27630234782332\n(1,)\n(1, 30)\nGradient Descent(): loss=23056779.67750019, w0=7.799359574713233, w1=10.250520789379634\n(1,)\n(1, 30)\nGradient Descent(): loss=63207196.74912305, w0=7.730142297660103, w1=10.174665399778739\n(1,)\n(1, 30)\nGradient Descent(): loss=20822791.764186762, w0=7.671776041580376, w1=10.147094154473656\n(1,)\n(1, 30)\nGradient Descent(): loss=11986062.882838417, w0=7.621400210968775, w1=10.126770303248671\n(1,)\n(1, 30)\nGradient Descent(): loss=19573498.927500077, w0=7.580527325568695, w1=10.123172981284862\n(1,)\n(1, 30)\nGradient Descent(): loss=355180.1508008595, w0=7.575565213445293, w1=10.120277611680445\n(1,)\n(1, 30)\nGradient Descent(): loss=24648226.258975327, w0=7.224859108175626, w1=10.158751370340028\n(1,)\n(1, 30)\nGradient Descent(): loss=6501644.732645616, w0=7.183619184692972, w1=10.14341574201244\n(1,)\n(1, 30)\nGradient Descent(): loss=325161.0446447267, w0=7.1810987017883825, w1=10.142293520284708\n(1,)\n(1, 30)\nGradient Descent(): loss=939246.8871893919, w0=7.173463740227945, w1=10.139331487976458\n(1,)\n(1, 30)\nGradient Descent(): loss=192657.75056047578, w0=7.169606350966858, w1=10.136966569808978\n(1,)\n(1, 30)\nGradient Descent(): loss=21872203.13022125, w0=7.137264770372391, w1=10.135962240678879\n(1,)\n(1, 30)\nGradient Descent(): loss=2384591.986955046, w0=7.12183720768364, w1=10.13192780331531\n(1,)\n(1, 30)\nGradient Descent(): loss=564181.5869746417, w0=7.114446217344656, w1=10.131564940626884\n(1,)\n(1, 30)\nGradient Descent(): loss=679699.444905409, w0=7.109387002491238, w1=10.127136845104253\n(1,)\n(1, 30)\nGradient Descent(): loss=32220595.69360827, w0=7.066066853856988, w1=10.124665570885636\n(1,)\n(1, 30)\nGradient Descent(): loss=56362293.1907262, w0=6.949454080133025, w1=10.123119178102627\n(1,)\n(1, 30)\nGradient Descent(): loss=22735254.659916833, w0=6.923091608412628, w1=10.122992743432015\n(1,)\n(1, 30)\nGradient Descent(): loss=212065.24932065868, w0=6.91839385986386, w1=10.120200170473753\n(1,)\n(1, 30)\nGradient Descent(): loss=132203228.31470658, w0=6.807842242698318, w1=10.077780382824464\n(1,)\n(1, 30)\nGradient Descent(): loss=764581.1554520437, w0=6.793364891681034, w1=10.073813021420507\n(1,)\n(1, 30)\nGradient Descent(): loss=16762346.407188423, w0=6.705954515342381, w1=10.051075200884117\n(1,)\n(1, 30)\nGradient Descent(): loss=252543415.6992112, w0=6.582746801785453, w1=10.03931448502108\n(1,)\n(1, 30)\nGradient Descent(): loss=1689871.4907642836, w0=6.593337317195359, w1=10.043736590974694\n(1,)\n(1, 30)\nGradient Descent(): loss=4733107.263137714, w0=6.6040443076408115, w1=10.047841398820326\n(1,)\n(1, 30)\nGradient Descent(): loss=14235.297790029419, w0=6.602607678491118, w1=10.047603486250098\n(1,)\n(1, 30)\nGradient Descent(): loss=37811019.12653944, w0=6.560576416058847, w1=10.030914378840766\n(1,)\n(1, 30)\nGradient Descent(): loss=1366369.3660824622, w0=6.5701941503141, w1=10.035668693677154\n(1,)\n(1, 30)\nGradient Descent(): loss=318130.5288941877, w0=6.56665561206692, w1=10.03337338789778\n(1,)\n(1, 30)\nGradient Descent(): loss=30869344.13062952, w0=6.1741786298646995, w1=10.056927899877694\n(1,)\n(1, 30)\nGradient Descent(): loss=50906067.417868316, w0=6.118355607714845, w1=10.043535177496901\n(1,)\n(1, 30)\nGradient Descent(): loss=7682541.598274936, w0=6.099628806150741, w1=10.031451707268728\n(1,)\n(1, 30)\nGradient Descent(): loss=2198140.547934791, w0=6.087800412442045, w1=10.029011007490022\n(1,)\n(1, 30)\nGradient Descent(): loss=15313631.85798146, w0=6.059643281662558, w1=10.01735351205866\n(1,)\n(1, 30)\nGradient Descent(): loss=1839911.3408020206, w0=6.047306206389903, w1=10.014005143161482\n(1,)\n(1, 30)\nGradient Descent(): loss=6217936.714627933, w0=6.006301311797818, w1=10.011371411455906\n(1,)\n(1, 30)\nGradient Descent(): loss=8236.61050149059, w0=6.007305757191185, w1=10.011402362587285\n(1,)\n(1, 30)\nGradient Descent(): loss=1490324.8889883861, w0=6.093542226551175, w1=10.003716440142435\n(1,)\n(1, 30)\nGradient Descent(): loss=489434.6897311296, w0=6.099801280890315, w1=10.00700325372395\n(1,)\n(1, 30)\nGradient Descent(): loss=14125952.810138566, w0=6.067470728047031, w1=10.00377078311736\n(1,)\n(1, 30)\nGradient Descent(): loss=451996.00838421634, w0=6.07149440240644, w1=10.003903893010039\n(1,)\n(1, 30)\nGradient Descent(): loss=17216483.925367404, w0=6.047754685451361, w1=9.990471249783926\n(1,)\n(1, 30)\nGradient Descent(): loss=12883207.67035267, w0=6.016730803752003, w1=9.97790088742909\n(1,)\n(1, 30)\nGradient Descent(): loss=28332.647381940667, w0=6.015378614452401, w1=9.97724696664614\n(1,)\n(1, 30)\nGradient Descent(): loss=21897084.754954565, w0=5.684823683445999, w1=10.009522230990711\n(1,)\n(1, 30)\nGradient Descent(): loss=3506282.3566897106, w0=5.55254985197774, w1=10.021819989942603\n(1,)\n(1, 30)\nGradient Descent(): loss=12583383.68443433, w0=5.519666720995162, w1=10.015848672616588\n(1,)\n(1, 30)\nGradient Descent(): loss=2903515.9831756996, w0=5.505389386383811, w1=10.010013394206295\n(1,)\n(1, 30)\nGradient Descent(): loss=13864697.59720952, w0=5.480796453319078, w1=10.009415717871441\n(1,)\n(1, 30)\nGradient Descent(): loss=143635.4014022636, w0=5.478053566808943, w1=10.009058409141291\n(1,)\n(1, 30)\nGradient Descent(): loss=18238380.632432934, w0=5.438194928396145, w1=9.985274164609237\n(1,)\n(1, 30)\nGradient Descent(): loss=6380965.868477892, w0=5.42398773345748, w1=9.979531556363243\n(1,)\n(1, 30)\nGradient Descent(): loss=15030576.377409091, w0=5.395494691444511, w1=9.963929411773456\n(1,)\n(1, 30)\nGradient Descent(): loss=4808186.5300788395, w0=5.372537472963528, w1=9.963101592417578\n(1,)\n(1, 30)\nGradient Descent(): loss=1500361.9648831312, w0=5.356927300609726, w1=9.955892013844995\n(1,)\n(1, 30)\nGradient Descent(): loss=9173012.310989136, w0=5.331657338253897, w1=9.952480424368082\n(1,)\n(1, 30)\nGradient Descent(): loss=331892.0414642942, w0=5.334612239773499, w1=9.953514071626149\n(1,)\n(1, 30)\nGradient Descent(): loss=7624847.707126159, w0=5.3137637674933105, w1=9.941764644606403\n(1,)\n(1, 30)\nGradient Descent(): loss=22130741.576870073, w0=5.275262926514784, w1=9.941183511134344\n(1,)\n(1, 30)\nGradient Descent(): loss=5249597.395732481, w0=5.1134126337974335, w1=9.95364177135365\n(1,)\n(1, 30)\nGradient Descent(): loss=10945816.079116974, w0=5.088952544652518, w1=9.945083921770555\n(1,)\n(1, 30)\nGradient Descent(): loss=610735.5979681861, w0=5.082389745367866, w1=9.94366976070478\n(1,)\n(1, 30)\nGradient Descent(): loss=27629568.72760049, w0=0.6286892324500001, w1=1.0285414953355\n(1,)\n(1, 30)\nGradient Descent(): loss=22949870.119425148, w0=0.6849862298635293, w1=1.052895353114745\n(1,)\n(1, 30)\nGradient Descent(): loss=8601907.441265633, w0=0.4778062087672549, w1=1.0748168243559255\n(1,)\n(1, 30)\nGradient Descent(): loss=3486036.6696173362, w0=0.4594875632142491, w1=1.07305470777325\n(1,)\n(1, 30)\nGradient Descent(): loss=723815.5000477922, w0=0.46637315544970986, w1=1.0757782757269445\n(1,)\n(1, 30)\nGradient Descent(): loss=630797.6487562322, w0=0.4572136783287343, w1=1.071848846282753\n(1,)\n(1, 30)\nGradient Descent(): loss=721214.082056325, w0=0.44963307509863554, w1=1.0679590108554782\n(1,)\n(1, 30)\nGradient Descent(): loss=354389.45000770636, w0=0.44363902823025386, w1=1.067787559941507\n(1,)\n(1, 30)\nGradient Descent(): loss=282224.7633512378, w0=0.4395028667013296, w1=1.0670947499741303\n(1,)\n(1, 30)\nGradient Descent(): loss=362405.95018625265, w0=0.4363192953980958, w1=1.066464148470064\n(1,)\n(1, 30)\nGradient Descent(): loss=251311.08621767192, w0=0.431905745074426, w1=1.0660976523670311\n(1,)\n(1, 30)\nGradient Descent(): loss=117285.33383221224, w0=0.4294738283649545, w1=1.0648995050627292\n(1,)\n(1, 30)\nGradient Descent(): loss=335740.77934469417, w0=0.435911347303684, w1=1.0674600495260433\n(1,)\n(1, 30)\nGradient Descent(): loss=150191.70451213655, w0=0.4410470053769847, w1=1.069447963104724\n(1,)\n(1, 30)\nGradient Descent(): loss=301002.243010048, w0=0.436273655703725, w1=1.067351548522766\n(1,)\n(1, 30)\nGradient Descent(): loss=62161.05948354541, w0=0.43489591454951243, w1=1.0664343296328223\n(1,)\n(1, 30)\nGradient Descent(): loss=1374767.0230024229, w0=0.4268325562399767, w1=1.059015834374734\n(1,)\n(1, 30)\nGradient Descent(): loss=247740.72779706013, w0=0.4237913019272564, w1=1.0588656915584482\n(1,)\n(1, 30)\nGradient Descent(): loss=128668.30899290394, w0=0.44913011613029546, w1=1.053888529563019\n(1,)\n(1, 30)\nGradient Descent(): loss=576358.9990374856, w0=0.4392968511680235, w1=1.0517549781359108\n(1,)\n(1, 30)\nGradient Descent(): loss=231121.57555577354, w0=0.43711751488413564, w1=1.051738762886766\n(1,)\n(1, 30)\nGradient Descent(): loss=130776.61291294212, w0=0.4352155592603549, w1=1.0509040442507656\n(1,)\n(1, 30)\nGradient Descent(): loss=294078.7822968168, w0=0.43847912477668854, w1=1.0527189853199073\n(1,)\n(1, 30)\nGradient Descent(): loss=250594.4551724568, w0=0.4738410758580816, w1=1.0502688516562917\n(1,)\n(1, 30)\nGradient Descent(): loss=35452.94253777822, w0=0.47551984814108395, w1=1.051181585061708\n(1,)\n(1, 30)\nGradient Descent(): loss=372921.12069400254, w0=0.47099580965123305, w1=1.0471896238301295\n(1,)\n(1, 30)\nGradient Descent(): loss=183911.01908465868, w0=0.4681836373896416, w1=1.0469720782571148\n(1,)\n(1, 30)\nGradient Descent(): loss=237525.61360199525, w0=0.43375611933355707, w1=1.0508994688879711\n(1,)\n(1, 30)\nGradient Descent(): loss=70212.7185545571, w0=0.4312499744809415, w1=1.0507434484613156\n(1,)\n(1, 30)\nGradient Descent(): loss=86.85088628894641, w0=0.43118189725046024, w1=1.050690796630963\n(1,)\n(1, 30)\nGradient Descent(): loss=130420.12611080488, w0=0.42695917004378037, w1=1.0475919968837213\n(1,)\n(1, 30)\nGradient Descent(): loss=93966.27776648696, w0=0.4240483111322659, w1=1.0474211498493442\n(1,)\n(1, 30)\nGradient Descent(): loss=271273.7995569378, w0=0.4209113714477494, w1=1.046685013491519\n(1,)\n(1, 30)\nGradient Descent(): loss=993529.3714799552, w0=0.4125401385041071, w1=1.0444739372389091\n(1,)\n(1, 30)\nGradient Descent(): loss=13349.12241320015, w0=0.41363677059402043, w1=1.0446594733691477\n(1,)\n(1, 30)\nGradient Descent(): loss=84146.24727372041, w0=0.4114114115365107, w1=1.0439322913713336\n(1,)\n(1, 30)\nGradient Descent(): loss=19239.43241585683, w0=0.40161320435599607, w1=1.044895771936069\n(1,)\n(1, 30)\nGradient Descent(): loss=79603.05390926002, w0=0.38168283463984487, w1=1.0460972400596082\n(1,)\n(1, 30)\nGradient Descent(): loss=316044.79901801265, w0=0.37747631443623114, w1=1.0449336583504636\n(1,)\n(1, 30)\nGradient Descent(): loss=347321.7141543061, w0=0.37336951785520855, w1=1.0440489066207486\n(1,)\n(1, 30)\nGradient Descent(): loss=145456.8236182021, w0=0.3708081855516781, w1=1.0432651026906\n(1,)\n(1, 30)\nGradient Descent(): loss=31959.800033665255, w0=0.3726771435670712, w1=1.0439233154030898\n(1,)\n(1, 30)\nGradient Descent(): loss=101944.18129063165, w0=0.3705976664510627, w1=1.0430802225107363\n(1,)\n(1, 30)\nGradient Descent(): loss=7084.399985756818, w0=0.3714294793404631, w1=1.0430891678232874\n(1,)\n(1, 30)\nGradient Descent(): loss=435960.6175901792, w0=0.3666710482374099, w1=1.0427758889184562\n(1,)\n(1, 30)\nGradient Descent(): loss=434609.55917135987, w0=0.3598305289262749, w1=1.0419348437547573\n(1,)\n(1, 30)\nGradient Descent(): loss=105370.96998324261, w0=0.3570698643985705, w1=1.0418361674366758\n(1,)\n(1, 30)\nGradient Descent(): loss=599680.7839948247, w0=0.3366294781799324, w1=1.0411136945584458\n(1,)\n(1, 30)\nGradient Descent(): loss=39857.46703892665, w0=0.338488054956034, w1=1.0413423039063843\n(1,)\n(1, 30)\nGradient Descent(): loss=62204.18934915589, w0=0.33655501409175353, w1=1.0409291679842736\n(1,)\n(1, 30)\nGradient Descent(): loss=228368.3884686247, w0=0.3027976534084836, w1=1.0436052582538546\n(1,)\n(1, 30)\nGradient Descent(): loss=33307.24311128583, w0=0.2899056671510626, w1=1.0447509286802423\n(1,)\n(1, 30)\nGradient Descent(): loss=105386.73710895219, w0=0.2874141963903295, w1=1.0445744274071347\n(1,)\n(1, 30)\nGradient Descent(): loss=347967.63305141695, w0=0.284042291166043, w1=1.0444191776979004\n(1,)\n(1, 30)\nGradient Descent(): loss=591352.2344708589, w0=0.2792568129879131, w1=1.0443694779347588\n(1,)\n(1, 30)\nGradient Descent(): loss=792180.0787671966, w0=0.26520591791945, w1=1.0336094898131012\n(1,)\n(1, 30)\nGradient Descent(): loss=255250.25667804334, w0=0.26217428760506156, w1=1.0330848017416971\n(1,)\n(1, 30)\nGradient Descent(): loss=48555.00275361514, w0=0.2466086296839985, w1=1.0346536143877834\n(1,)\n(1, 30)\nGradient Descent(): loss=299177.29595131235, w0=0.24164954295274, w1=1.0341626524635241\n(1,)\n(1, 30)\nGradient Descent(): loss=67899.16477750054, w0=0.24354019294946563, w1=1.0352905640865426\n(1,)\n(1, 30)\nGradient Descent(): loss=379493.7046035031, w0=0.23128412418599714, w1=1.03135039311782\n(1,)\n(1, 30)\nGradient Descent(): loss=18951.07411990498, w0=0.23221180670940514, w1=1.0316146484505337\n(1,)\n(1, 30)\nGradient Descent(): loss=92190.83398804665, w0=0.23025631191473842, w1=1.0308218959577167\n(1,)\n(1, 30)\nGradient Descent(): loss=12127.809430655045, w0=0.22950225512260652, w1=1.0307475758013243\n(1,)\n(1, 30)\nGradient Descent(): loss=3773.4364356339497, w0=0.22993495086359508, w1=1.0307541477240687\n(1,)\n(1, 30)\nGradient Descent(): loss=59233.92810559955, w0=0.22820117173600316, w1=1.0302222684475055\n(1,)\n(1, 30)\nGradient Descent(): loss=22663.564407351965, w0=0.2291488716655244, w1=1.0303321044284193\n(1,)\n(1, 30)\nGradient Descent(): loss=4582.381629365841, w0=0.22886264504303278, w1=1.0300748273955864\n(1,)\n(1, 30)\nGradient Descent(): loss=27.823816013026175, w0=0.2288082799932577, w1=1.030073185508123\n(1,)\n(1, 30)\nGradient Descent(): loss=164305.6992707816, w0=0.22514199685910932, w1=1.0287951900550776\n(1,)\n(1, 30)\nGradient Descent(): loss=156422.61701719777, w0=0.2290289189172568, w1=1.0303349857294588\n(1,)\n(1, 30)\nGradient Descent(): loss=217068.62064732355, w0=0.2258869646321394, w1=1.029697969990324\n(1,)\n(1, 30)\nGradient Descent(): loss=19310.933881087687, w0=0.2247765817176748, w1=1.0289167066077\n(1,)\n(1, 30)\nGradient Descent(): loss=11170.665945400933, w0=0.2173105466976069, w1=1.0298027134642909\n(1,)\n(1, 30)\nGradient Descent(): loss=9728.266912242092, w0=0.2181730765304382, w1=1.0300932153978484\n(1,)\n(1, 30)\nGradient Descent(): loss=13110.93300079439, w0=0.21696202195100903, w1=1.0296384788799642\n(1,)\n(1, 30)\nGradient Descent(): loss=611.7265909701937, w0=0.2167852908867092, w1=1.0296315147809465\n(1,)\n(1, 30)\nGradient Descent(): loss=13071.208449857546, w0=0.20870906679863244, w1=1.0303586983212092\n(1,)\n(1, 30)\nGradient Descent(): loss=14411.5385138733, w0=0.2094579211938759, w1=1.0307177015867972\n(1,)\n(1, 30)\nGradient Descent(): loss=17160.57418522145, w0=0.20811830846073998, w1=1.0306761015260553\n(1,)\n(1, 30)\nGradient Descent(): loss=216868.25748041563, w0=0.2052549708223976, w1=1.0295417516978056\n(1,)\n(1, 30)\nGradient Descent(): loss=30469.37259708992, w0=0.21758551507666074, w1=1.0282825550374137\n(1,)\n(1, 30)\nGradient Descent(): loss=19692.783179996182, w0=0.216155000251062, w1=1.0275847668875353\n(1,)\n(1, 30)\nGradient Descent(): loss=760720.7818912002, w0=0.2086888843977344, w1=1.0271413353955734\n(1,)\n(1, 30)\nGradient Descent(): loss=39018.4204037821, w0=0.19473531449942613, w1=1.0287086048361633\n(1,)\n(1, 30)\nGradient Descent(): loss=353.12005852790526, w0=0.19485006744933386, w1=1.0287957621206238\n(1,)\n(1, 30)\nGradient Descent(): loss=15661.19275279442, w0=0.19353975974392892, w1=1.0284914604493303\n(1,)\n(1, 30)\nGradient Descent(): loss=69.20144304659482, w0=0.19348691371954324, w1=1.0284593293135773\n(1,)\n(1, 30)\nGradient Descent(): loss=45.01983025822147, w0=0.19301294201354416, w1=1.0285379341287273\n(1,)\n(1, 30)\nGradient Descent(): loss=26193.180936179877, w0=0.19066147921278936, w1=1.0270475776350307\n(1,)\n(1, 30)\nGradient Descent(): loss=31406.7422366332, w0=0.19199957241307294, w1=1.0270493696122516\n(1,)\n(1, 30)\nGradient Descent(): loss=12250.12958054535, w0=0.19095819203497663, w1=1.0267813978085358\n(1,)\n(1, 30)\nGradient Descent(): loss=842461.2927614221, w0=0.18413462550808615, w1=1.0247476847405086\n(1,)\n(1, 30)\nGradient Descent(): loss=255.51670315968082, w0=0.1842363730695434, w1=1.0247814706095517\n(1,)\n(1, 30)\nGradient Descent(): loss=540.9479861975005, w0=0.18397449040966027, w1=1.0246598928629735\n(1,)\n(1, 30)\nGradient Descent(): loss=6924.448351121667, w0=0.1844709879637257, w1=1.0248502483070547\n(1,)\n(1, 30)\nGradient Descent(): loss=4700.046812593369, w0=0.18485260407023948, w1=1.0248860243625681\n(1,)\n(1, 30)\nGradient Descent(): loss=2703.9927499322343, w0=0.18448559662667466, w1=1.0246551412972729\n(1,)\n(1, 30)\nGradient Descent(): loss=3181.3037850143055, w0=0.18420343652591137, w1=1.0244954737852376\n(1,)\n(1, 30)\nGradient Descent(): loss=3335.9800049279047, w0=0.18473551759207874, w1=1.0247669114608742\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name &#39;losses&#39; is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-186-56fec3e8e96a&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mweightss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----&gt; 8\u001b[0;31m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweightss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name &#39;losses&#39; is not defined"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "mls=[]\n",
    "weightss=[]\n",
    "for i in (-10,10,1):\n",
    "    ml,weights=get_w(y,tX,2,i,max_iters=100,log=True)\n",
    "    mls.append(ml)\n",
    "    weightss.append(weights)\n",
    "weights=weightss[np.where(mls==min(mls))]\n",
    "\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}