{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import importlib.util\n",
    "import implementations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1. 0. 0. ... 1. 0. 0.]\n"
    }
   ],
   "source": [
    "# normalize datas\n",
    "def normalize (tx):\n",
    "    tmp=[]\n",
    "    for column in tx.T:\n",
    "        min = column.min()\n",
    "        max = column.max()\n",
    "        new_col=(column - min)/(max-min)\n",
    "        tmp.append(new_col)\n",
    "    return np.array(tmp).T\n",
    "\n",
    "tX_test = normalize(tX_test)\n",
    "tX = normalize(tX)\n",
    "y[y==-1]=0\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#apply method\n",
    "def get_w_loss(y,x,method,initial_w=0,max_iters=10,gamma=0.00000005,threshold=1e-08,logs=False,batch_size=1,lambda_=0.005, store=False):\n",
    "    \"\"\"applies the algorithm described by <method>. by default as the awaited behaviour for the project, but offers some possible enhancements (batch_size,threshold, logs, store)\n",
    "    Args:\n",
    "        y (numpy.ndarray): An array with shape (n,1)\n",
    "        tx (numpy.ndarray): An array with shape (n,m)\n",
    "        method (int): 1 -> least_squares_GD*                \n",
    "                      2 -> stochastic_gradient_descent*\n",
    "                      3 -> least_squares\n",
    "                      4 -> ridge_regression\n",
    "                      5 -> logistic_regression_gradient_descent*\n",
    "                      6 -> reg_logistic_regression*\n",
    "        * this algorithm is an iterative descent\n",
    "        initial_w (Union[float, numpy.ndarray], optional): if a int is set w will start with an array. Defaults to 0.\n",
    "        max_iters (int, optional): number of iterations (only for descent algorithms). Defaults to 10.\n",
    "        gamma (float, optional): descent scale factor (only for descent algorithms). Defaults to 0.00000005.\n",
    "        threshold (float, optional): threshold to prevent divergence in case of linear separability (I assume). Defaults to 1e-08.\n",
    "        log (bool, optional): if True step by step logs are displayed (only for descent algorithms). Defaults to False.\n",
    "        batch_size (int, optional): size of the sample on which to compute the partial gradient for stochastic gradient descent algorithm. Defaults to 1.\n",
    "        lambda_ (float, optional): regularization factor with good values allows to limit overfitting and underfitting (only for ridge_regression and reg_logistic_regression). Defaults to 0.005.\n",
    "        store (bool, optional): [description]. Defaults to False.\n",
    "    Returns:\n",
    "        w (numpy.ndarray): ndarray with shape (n_iters,m) if store and if the method is a descent, else shape(m,1) \n",
    "        loss (Union[numpy.ndarray, float]): ndarray with shape (n_iters,) if store and if the method is a descent, else float\n",
    "    :raises ValueError: if method is not <6\n",
    "    \"\"\"\n",
    "    initial_=np.array([initial_w]*len(x[0]))\n",
    "    if(method==1):\n",
    "        return implementations.least_squares_GD(y,x, initial_, max_iters, gamma, logs)\n",
    "    elif(method==2): \n",
    "        return implementations.stochastic_gradient_descent(y,x, initial_, max_iters, gamma, batch_size, logs)\n",
    "    elif(method==3):\n",
    "        return implementations.least_squares(y,x)\n",
    "    elif(method==4):\n",
    "        return implementations.ridge_regression(y,x, lambda_=lambda_)\n",
    "    elif(method==5):\n",
    "        return implementations.logistic_regression_gradient_descent(y,x,initial_w,max_iters,gamma,threshold,logs,store)\n",
    "    elif(method==6):\n",
    "        return implementations.reg_logistic_regression(y,x,initial_w,max_iters,gamma,threshold,logs,store,lambda_)\n",
    "    else:\n",
    "        return ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1.29403870e-01 -2.91730995e-02 -2.00207353e-02 -7.43987553e-03\n  2.61200030e-05  2.96128267e-03  4.45894474e-03  2.63959264e-03\n -5.03080879e-02 -9.20242310e-04 -1.89036279e-03 -1.40595144e-02\n  1.11295115e-02  2.89695886e-03  9.93541798e-04 -6.35308664e-02\n -6.45185187e-02 -5.71741926e-03 -6.28745791e-02 -6.41306668e-02\n -1.71977192e-03 -6.22424195e-02 -7.33113073e-03 -1.40323921e-02\n -1.41680560e-02 -2.98315112e-02 -2.98759455e-02  1.69084100e-03\n  2.86435526e-03  2.86468641e-03 -6.81434900e-04]163801.4252998072\n"
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "\n",
    "#for i in range(-10,10,1):\n",
    "#    ml,weights=get_w(y,tX,2,i,max_iters=1000,log=False)\n",
    "#    mls.append(ml)\n",
    "#    weightss.append(weights)\n",
    "#print(np.where(mls==min(mls)))\n",
    "#print(mls)\n",
    "method=6\n",
    "weights, loss=get_w_loss(y,tX,method,logs=False,max_iters=100, gamma=0.00000005)\n",
    "print(str(weights) + str(loss))\n",
    "#weights=weightss[np.where(mls==min(mls))[0][0]]\n",
    "\n",
    "if(method==5 or method==6 ):\n",
    "    data = np.c_[np.ones((_.shape[0], 1)), tX_test]\n",
    "else:\n",
    "    data=tX_test\n",
    "y_pred = predict_labels(weights, data)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}